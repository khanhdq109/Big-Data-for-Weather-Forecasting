{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import libraries"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import findspark\n","findspark.init()\n","findspark.find() \n","\n","import databricks.koalas as ks\n","from pyspark.ml import Pipeline\n","from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n","from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.sql import SparkSession"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["DATASET_PATH = 'datasets/historical-hourly-weather-dataset/'\n","AGGREGATED_DATASET_PATH = 'datasets/historical-hourly-weather-dataset/aggregated_sampled_weather_measurements'"]},{"cell_type":"markdown","metadata":{},"source":["# Pre-processing dataset"]},{"cell_type":"markdown","metadata":{},"source":["### Load dataset into Koalas `DataFrame` objects"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["city_attributes_df = ks.read_csv(os.path.join(DATASET_PATH, 'city_attributes.csv'))\n","humidity_df = ks.read_csv(os.path.join(DATASET_PATH, 'humidity.csv'))\n","pressure_df = ks.read_csv(os.path.join(DATASET_PATH, 'pressure.csv'))\n","temperature_df = ks.read_csv(os.path.join(DATASET_PATH, 'temperature.csv'))\n","weather_description_df = ks.read_csv(os.path.join(DATASET_PATH, 'weather_description.csv'))\n","wind_direction_df = ks.read_csv(os.path.join(DATASET_PATH, 'wind_direction.csv'))\n","wind_speed_df = ks.read_csv(os.path.join(DATASET_PATH, 'wind_speed.csv'))"]},{"cell_type":"markdown","metadata":{},"source":["### Combine all data into one `DataFrame`"]},{"cell_type":"markdown","metadata":{},"source":["# Machine Learning pipeline"]},{"cell_type":"markdown","metadata":{},"source":["### Load data"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["your 131072x1 screen size is bogus. expect trouble\n","23/11/10 16:39:20 WARN Utils: Your hostname, DASH resolves to a loopback address: 127.0.1.1; using 172.28.34.48 instead (on interface eth0)\n","23/11/10 16:39:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","23/11/10 16:39:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","                                                                                \r"]}],"source":["# Get all the csv files in the aggregated dataset folder\n","csv_files = [file for file in os.listdir(AGGREGATED_DATASET_PATH) if file.endswith('.csv')]\n","\n","# Read each CSV file into a Koalas DataFrame and store them in a list\n","dfs = [ks.read_csv(os.path.join(AGGREGATED_DATASET_PATH, file)) for file in csv_files]\n","\n","# Combine the DataFrames using the concat function\n","data = ks.concat(dfs, ignore_index = True)"]},{"cell_type":"markdown","metadata":{},"source":["### Pre-processing"]},{"cell_type":"markdown","metadata":{},"source":["Select relevant features and label column"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Select relevant features\n","numerical_cols = [\n","    'humidity',\n","    'pressure',\n","    'temperature',\n","    'wind_direction',\n","    'wind_speed',\n","    'latitude',\n","    'longitude'\n","]\n","nominal_cols = []\n","# Select the label column\n","label_col = 'weather_condition'\n","# Select the features and the label\n","df_selected = data[numerical_cols + [label_col]]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["spark = SparkSession.builder.getOrCreate()\n","df_selected = df_selected.to_spark()"]},{"cell_type":"markdown","metadata":{},"source":["Train-Test split"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["train_data, test_data = df_selected.randomSplit([0.8, 0.2], seed = 42)"]},{"cell_type":"markdown","metadata":{},"source":["Encode"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["def encode(\n","    df,\n","    numerical_cols = [],\n","    nominal_cols = [],\n","    label_col = '',\n","    with_std = True,\n","    with_mean = True,\n","):\n","    # Convert categorical label to numerical label\n","    label_indexer = StringIndexer(\n","        inputCol = label_col,\n","        outputCol = 'label',\n","        handleInvalid = 'keep'\n","    )\n","    \n","    # Assemble features into a vector\n","    feature_cols = numerical_cols + nominal_cols\n","    vector_assembler = VectorAssembler(\n","        inputCols = feature_cols,\n","        outputCol = 'raw_features'\n","    )\n","    \n","    # Scale the features\n","    scaler = StandardScaler(\n","        inputCol = 'raw_features',\n","        outputCol = 'scaled_features',\n","        withStd = with_std,\n","        withMean = with_mean\n","    )\n","    \n","    stages = [label_indexer, vector_assembler, scaler]\n","    pipeline = Pipeline(stages = stages)\n","    \n","    transformer = pipeline.fit(df)\n","    \n","    return transformer\n","\n","data_encoder = encode(\n","    df = df_selected,\n","    numerical_cols = numerical_cols,\n","    nominal_cols = nominal_cols,\n","    label_col = label_col\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Build a `PySpark` Machine Learning pipeline"]},{"cell_type":"markdown","metadata":{},"source":["Define the classifier"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["mode = 'LogisticRegression'"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["if mode == 'RandomForest':\n","    classifier = RandomForestClassifier(\n","        featuresCol = 'scaled_features',\n","        labelCol = 'label',\n","        numTrees = 30\n","    )\n","elif mode == 'LogisticRegression':\n","    classifier = LogisticRegression(\n","        featuresCol = 'scaled_features',\n","        labelCol = 'label'\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["Define the pipeline with the encoding and classifier stages"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["pipeline = Pipeline(stages = [data_encoder, classifier])"]},{"cell_type":"markdown","metadata":{},"source":["Define the evaluator"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["evaluator = MulticlassClassificationEvaluator(\n","    labelCol = 'label',\n","    predictionCol = 'prediction',\n","    metricName = 'accuracy'\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Define hyperparameter tuning (optional)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["# Define the grid of hyperparameters\n","param_grid = ParamGridBuilder().build()\n","\n","# Set up the cross validator for model training and hyperparameter tuning\n","cross_validator = CrossValidator(\n","    estimator = pipeline,\n","    estimatorParamMaps = param_grid,\n","    evaluator = evaluator,\n","    numFolds = 5\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Start training"]},{"cell_type":"markdown","metadata":{},"source":["Fit the model using the training data"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/11/10 16:47:02 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"]}],"source":["model = cross_validator.fit(train_data)"]},{"cell_type":"markdown","metadata":{},"source":["Make predictions on the test data"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["predictions = model.transform(test_data)"]},{"cell_type":"markdown","metadata":{},"source":["Evaluate the model performance"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.48947995236204844\n"]}],"source":["accuracy = evaluator.evaluate(predictions)\n","print('Accuracy:', accuracy)"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"Weather_forecasting_pyspark","notebookOrigID":1252952709241176,"widgets":{}},"kernelspec":{"display_name":"weather","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
