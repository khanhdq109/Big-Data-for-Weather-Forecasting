{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Data pre-processing"]},{"cell_type":"markdown","metadata":{},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import requests\n","import calendar\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from dotenv import load_dotenv\n","from datetime import datetime, timedelta\n","from IPython.display import clear_output\n","\n","import pyspark.pandas as ps\n","from pyspark.sql import *\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","\n","load_dotenv()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DATASET_PATH = 'datasets/historical-hourly-weather-dataset/'\n","AGGREGATED_DATASET_PATH = 'datasets/historical-hourly-weather-dataset/tmp'"]},{"cell_type":"markdown","metadata":{},"source":["### Data expansion"]},{"cell_type":"markdown","metadata":{},"source":["Support functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def call_api_to_get_data(date, enddate, city = \"Ho+Chi+Minh+City\"):\n","\n","    link = \"http://api.worldweatheronline.com/premium/v1/past-weather.ashx?q={city}&date={date}&enddate={enddate}&key={api_key}&format=json&tp=1\".format(\n","        date =date,\n","        enddate = enddate,\n","        city = city,\n","        api_key = os.getenv(\"api_key\")\n","    )\n","    response = requests.get(link)\n","    return response.json()\n","\n","def get_lst_first_day_last_day(start_date = datetime(2012, 10, 1), end_date = datetime(2017, 11, 30)):\n","\n","    date_format = \"%Y-%m-%d\"\n","\n","    current_date = start_date\n","    dates_list = []\n","\n","    while current_date <= end_date:\n","        last_day_of_month = calendar.monthrange(current_date.year, current_date.month)[1]\n","\n","        first_day_of_month = current_date.replace(day = 1)\n","        last_day_of_month = current_date.replace(day = last_day_of_month)\n","\n","        dates_list.append((first_day_of_month.strftime(date_format), last_day_of_month.strftime(date_format)))\n","\n","        current_date = last_day_of_month + timedelta(days=1)\n","\n","    return dates_list\n","\n","def handle_append_data(json, humidity_arr, time_arr, wind_dir_arr, wind_speed_arr, pressure_arr, weather_desc_arr , temp_arr):\n","\n","    for day_data in json[\"data\"][\"weather\"]:\n","\n","        date = day_data[\"date\"]\n","        for hour_data in day_data['hourly']:\n","            time = \"{date} {time}:00:00\".format(date = date, time = int(int(hour_data[\"time\"])/100))\n","            wind_speed_ms = round(float(hour_data[\"windspeedKmph\"])* (10/36),2)\n","            wind_dir = int(hour_data[\"winddirDegree\"])\n","            humidity = int(hour_data[\"humidity\"])\n","            pressure = int(hour_data[\"pressure\"])\n","            tempK = int(hour_data[\"tempC\"]) + 273.15\n","            weather_desc = hour_data[\"weatherDesc\"][0][\"value\"]\n","\n","            time_arr = np.append(time_arr, time)\n","            humidity_arr = np.append(humidity_arr ,  humidity)\n","            wind_speed_arr = np.append(wind_speed_arr, wind_speed_ms)\n","            wind_dir_arr = np.append(wind_dir_arr, wind_dir)\n","            pressure_arr = np.append(pressure_arr, pressure)\n","            temp_arr = np.append(temp_arr, tempK)\n","            weather_desc_arr = np.append(weather_desc_arr, weather_desc)\n","\n","    return humidity_arr, time_arr, wind_dir_arr, wind_speed_arr, pressure_arr, weather_desc_arr , temp_arr"]},{"cell_type":"markdown","metadata":{},"source":["Read data into DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_humidity = pd.read_csv(\"historical-hourly-weather-dataset/humidity.csv\")\n","df_pressure = pd.read_csv(\"historical-hourly-weather-dataset/pressure.csv\")\n","df_temperature = pd.read_csv(\"historical-hourly-weather-dataset/temperature.csv\")\n","df_weather_desc = pd.read_csv(\"historical-hourly-weather-dataset/weather_description.csv\")\n","df_wind_dir = pd.read_csv(\"historical-hourly-weather-dataset/wind_direction.csv\")\n","df_wind_speed = pd.read_csv(\"historical-hourly-weather-dataset/wind_speed.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["Crawl Ho Chi Minh city to merge old data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["humidity_arr = np.array([])\n","time_arr = np.array([])\n","wind_dir_arr = np.array([])\n","wind_speed_arr = np.array([])\n","pressure_arr = np.array([])\n","temp_arr = np.array([])\n","weather_desc_arr = np.array([], dtype = object)\n","count = 0\n","dates_list = get_lst_first_day_last_day()\n","for date in dates_list:\n","    clear_output(wait = True)\n","    print(count / len(dates_list) * 100)\n","    count += 1\n","    json = call_api_to_get_data(date[0], date[1])\n","    humidity_arr, time_arr, wind_dir_arr, wind_speed_arr, pressure_arr, weather_desc_arr , temp_arr = handle_append_data(\n","        json, humidity_arr, time_arr, wind_dir_arr, wind_speed_arr, pressure_arr, weather_desc_arr , temp_arr)\n","\n","df_hcm_humidity = pd.DataFrame({\"datetime\": time_arr, \"Ho Chi Minh City\": humidity_arr})\n","df_hcm_pressure = pd.DataFrame({\"datetime\": time_arr, \"Ho Chi Minh City\": pressure_arr})\n","df_hcm_wind_dir = pd.DataFrame({\"datetime\": time_arr, \"Ho Chi Minh City\": wind_dir_arr})\n","df_hcm_wind_speed = pd.DataFrame({\"datetime\": time_arr, \"Ho Chi Minh City\": wind_speed_arr})\n","df_hcm_temp= pd.DataFrame({\"datetime\": time_arr, \"Ho Chi Minh City\": temp_arr})\n","df_hcm_weather_desc = pd.DataFrame({\"datetime\": time_arr, \"Ho Chi Minh City\": weather_desc_arr})\n","\n","df_humidity = pd.merge(df_humidity, df_hcm_humidity, on = \"datetime\")\n","df_pressure = pd.merge(df_pressure, df_hcm_pressure, on = \"datetime\")\n","df_wind_dir = pd.merge(df_wind_dir, df_hcm_wind_dir, on = \"datetime\")\n","df_wind_speed = pd.merge(df_wind_speed, df_hcm_wind_speed, on = \"datetime\")\n","df_temperature = pd.merge(df_temperature, df_hcm_temp, on = \"datetime\")\n","df_weather_desc = pd.merge(df_weather_desc, df_hcm_weather_desc, on = \"datetime\")"]},{"cell_type":"markdown","metadata":{},"source":["Crawl data from 2023 to now"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_country = pd.read_csv(\"historical-hourly-weather-dataset/city_attributes.csv\")\n","\n","lst_city = list(df_country.City.unique())\n","lst_city.append(\"Ho Chi Minh City\")\n","\n","df_new_humidity = pd.DataFrame()\n","df_new_pressure = pd.DataFrame()\n","df_new_temperature = pd.DataFrame()\n","df_new_weather_desc = pd.DataFrame()\n","df_new_wind_dir = pd.DataFrame()\n","df_new_wind_speed = pd.DataFrame()\n","\n","init = True\n","\n","for city in lst_city:\n","\n","    lst_dates = get_lst_first_day_last_day(datetime(2023, 1, 1), datetime(2023, 10, 1))\n","\n","    humidity_arr = np.array([])\n","    time_arr = np.array([])\n","    wind_dir_arr = np.array([])\n","    wind_speed_arr = np.array([])\n","    pressure_arr = np.array([])\n","    temp_arr = np.array([])\n","    weather_desc_arr = np.array([], dtype = object)\n","    count = 0\n","    for date in lst_dates:\n","        print(city, count/ len(lst_dates) * 100)\n","        count += 1\n","        json = call_api_to_get_data(date[0], date[1], city)\n","        humidity_arr, time_arr, wind_dir_arr, wind_speed_arr, pressure_arr, weather_desc_arr , temp_arr = handle_append_data(\n","            json, humidity_arr, time_arr, wind_dir_arr, wind_speed_arr, pressure_arr, weather_desc_arr , temp_arr)\n","\n","    df_city_humidity = pd.DataFrame({\"datetime\": time_arr, city: humidity_arr})\n","    df_city_pressure = pd.DataFrame({\"datetime\": time_arr, city: pressure_arr})\n","    df_city_wind_dir = pd.DataFrame({\"datetime\": time_arr, city: wind_dir_arr})\n","    df_city_wind_speed = pd.DataFrame({\"datetime\": time_arr, city: wind_speed_arr})\n","    df_city_temp= pd.DataFrame({\"datetime\": time_arr, city: temp_arr})\n","    df_city_weather_desc = pd.DataFrame({\"datetime\": time_arr , city: weather_desc_arr})\n","    if init:\n","        df_new_humidity = df_city_humidity.copy()\n","        df_new_pressure = df_city_pressure.copy()\n","        df_new_wind_dir = df_city_wind_dir.copy()\n","        df_new_wind_speed = df_city_wind_speed.copy()\n","        df_new_temperature = df_city_temp.copy()\n","        df_new_weather_desc = df_city_weather_desc.copy()\n","        init = False\n","    else :\n","        df_new_humidity = pd.merge(df_new_humidity, df_city_humidity, on = \"datetime\")\n","        df_new_pressure = pd.merge(df_new_pressure, df_city_pressure, on = \"datetime\")\n","        df_new_wind_dir = pd.merge(df_new_wind_dir, df_city_wind_dir, on = \"datetime\")\n","        df_new_wind_speed = pd.merge(df_new_wind_speed, df_city_wind_speed, on = \"datetime\")\n","        df_new_temperature = pd.merge(df_new_temperature, df_city_temp, on = \"datetime\")\n","        df_new_weather_desc = pd.merge(df_new_weather_desc, df_city_weather_desc, on = \"datetime\")"]},{"cell_type":"markdown","metadata":{},"source":["Merge into one DataFrame per attribute"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_humidity = pd.concat([df_humidity, df_new_humidity], axis = 0)\n","df_pressure = pd.concat([df_pressure, df_new_pressure], axis = 0)\n","df_wind_dir = pd.concat([df_wind_dir, df_new_wind_dir], axis = 0)\n","df_wind_speed = pd.concat([df_wind_speed, df_new_wind_speed], axis = 0)\n","df_temperature = pd.concat([df_temperature, df_new_temperature], axis = 0)\n","df_weather_desc = pd.concat([df_weather_desc, df_new_weather_desc], axis = 0)"]},{"cell_type":"markdown","metadata":{},"source":["Save to .csv files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_humidity.to_csv(\"historical-hourly-weather-dataset/humidity.csv\", index = False)\n","df_pressure.to_csv(\"historical-hourly-weather-dataset/pressure.csv\", index = False)\n","df_wind_dir.to_csv(\"historical-hourly-weather-dataset/wind_direction.csv\", index = False)\n","df_wind_speed.to_csv(\"historical-hourly-weather-dataset/wind_speed.csv\", index = False)\n","df_temperature.to_csv(\"historical-hourly-weather-dataset/temperature.csv\", index = False)\n","df_weather_desc.to_csv(\"historical-hourly-weather-dataset/weather_description.csv\", index = False)"]},{"cell_type":"markdown","metadata":{},"source":["### Data cleaning & integration"]},{"cell_type":"markdown","metadata":{},"source":["Load data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weather_conditions_df = ps.read_csv(f'{DATASET_PATH}weather_description.csv')\n","humidity_df = ps.read_csv(f'{DATASET_PATH}humidity.csv')\n","pressure_df = ps.read_csv(f'{DATASET_PATH}pressure.csv')\n","temperature_df = ps.read_csv(f'{DATASET_PATH}temperature.csv')\n","city_attributes_df = ps.read_csv(f'{DATASET_PATH}city_attributes.csv')\n","wind_direction_df = ps.read_csv(f'{DATASET_PATH}wind_direction.csv')\n","wind_speed_df = ps.read_csv(f'{DATASET_PATH}wind_speed.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DATETIME_COL = 'datetime'\n","HUMIDITY_COL = 'humidity'\n","PRESSURE_COL = 'pressure'\n","TEMPERATURE_COL = 'temperature'\n","WIND_DIRECTION_COL = 'wind_direction'\n","WIND_SPEED_COL = 'wind_speed'\n","LATITUDE_COL = 'latitude'\n","LONGITUDE_COL = 'longitude'\n","CITY_COL = 'city'\n","COUNTRY_COL = 'country'\n","WEATHER_CONDITION_COL = 'weather_condition'"]},{"cell_type":"markdown","metadata":{},"source":["Create a single `DataFrame` that includes all data from the others"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def filter_dataframe_by_city_column(dataframe, city_name, new_column_name):\n","    return dataframe.to_spark() \\\n","        .withColumn(new_column_name, col(city_name)) \\\n","        .select([DATETIME_COL, new_column_name])\n","        \n","\n","def join_dataframes(dataframes: List[DataFrame], column_name: str) -> DataFrame:\n","    joined_df = dataframes[0]\n","\n","    for dataframe in dataframes[1:]:\n","        joined_df = joined_df.join(dataframe, [column_name])\n","\n","    return joined_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weather_measurements_df = None\n","\n","# Iterate over all the records in the cities `DataFrame`\n","for index, row in city_attributes_df.iterrows():    \n","\n","    city = row.City\n","    country = row.Country\n","    latitude = row.Latitude\n","    longitude = row.Longitude\n","\n","    # Compute a list of `DataFrame`, one for each type of measurement in the city\n","    dataframes = [\n","        filter_dataframe_by_city_column(humidity_df, city, HUMIDITY_COL),\n","        filter_dataframe_by_city_column(pressure_df, city, PRESSURE_COL),\n","        filter_dataframe_by_city_column(temperature_df, city, TEMPERATURE_COL),\n","        filter_dataframe_by_city_column(wind_direction_df, city, WIND_DIRECTION_COL),\n","        filter_dataframe_by_city_column(wind_speed_df, city, WIND_SPEED_COL),\n","        filter_dataframe_by_city_column(weather_conditions_df, city, WEATHER_CONDITION_COL)\n","    ]\n","\n","    # Compute a `DataFrame` that includes all the data about the measurements in the city\n","    joined_df = join_dataframes(dataframes, DATETIME_COL) \\\n","        .withColumn(CITY_COL, lit(city)) \\\n","        .withColumn(COUNTRY_COL, lit(country)) \\\n","        .withColumn(LATITUDE_COL, lit(latitude)) \\\n","        .withColumn(LONGITUDE_COL, lit(longitude))\n","\n","    # Union the `DataFrame` with the ones computed in the previous iterations\n","    weather_measurements_df = weather_measurements_df.union(joined_df) if weather_measurements_df is not None else joined_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weather_measurements_df.count()"]},{"cell_type":"markdown","metadata":{},"source":["Clean missing value"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["not_null_weather_measurements_df = weather_measurements_df.dropna()"]},{"cell_type":"markdown","metadata":{},"source":["### Label aggregation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_weather_conditions_aggregation_dict(weather_conditions):\n","    weather_conditions_dict = dict()\n","  \n","    for weather_condition in weather_conditions:\n","  \n","        weather_condition_lowered = weather_condition.lower()\n","\n","        if any(key in weather_condition_lowered for key in ['squall', 'thunderstorm']):\n","            weather_conditions_dict[weather_condition] = 'thunderstorm'\n","        elif any(key in weather_condition_lowered for key in ['drizzle', 'rain']):\n","            weather_conditions_dict[weather_condition] = 'rainy'\n","        elif any(key in weather_condition_lowered for key in ['sleet', 'snow']):\n","            weather_conditions_dict[weather_condition] = 'snowy'\n","        elif any(key in weather_condition_lowered for key in ['smoke', 'cloud']):\n","            weather_conditions_dict[weather_condition] = 'cloudy'\n","        elif any(key in weather_condition_lowered for key in ['fog', 'mist', 'haze']):\n","            weather_conditions_dict[weather_condition] = 'foggy'\n","        elif any(key in weather_condition_lowered for key in ['clear', 'sun']):\n","            weather_conditions_dict[weather_condition] = 'sunny'\n","            \n","    return weather_conditions_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weather_conditions_all = not_null_weather_measurements_df \\\n","    .select(col(WEATHER_CONDITION_COL)).distinct() \\\n","    .toPandas()[WEATHER_CONDITION_COL].to_numpy().reshape(-1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weather_conditions_dict = get_weather_conditions_aggregation_dict(weather_conditions_all)"]},{"cell_type":"markdown","metadata":{},"source":["Replace all the weather conditions in the `DataFrame` with the aggregated ones"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weather_measurements_aggregated_df = not_null_weather_measurements_df.replace(weather_conditions_dict)"]},{"cell_type":"markdown","metadata":{},"source":["Remove all samples that contain other conditions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["WEATHER_CONDITIONS = set(weather_conditions_dict.values())\n","\n","weather_measurements_aggregated_df = weather_measurements_aggregated_df \\\n","    .filter(weather_measurements_aggregated_df[WEATHER_CONDITION_COL].isin(WEATHER_CONDITIONS))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get all unique value from weather_measurements_aggregated_df in column weather_condition\n","weather_conditions = weather_measurements_aggregated_df \\\n","    .select(col(WEATHER_CONDITION_COL)).distinct() \\\n","    .toPandas()[WEATHER_CONDITION_COL].to_numpy().reshape(-1)\n","    \n","print(weather_conditions)"]},{"cell_type":"markdown","metadata":{},"source":["### Undersampling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def count_weather_condition_occurrences(dataframe: DataFrame, class_name: str) -> int:\n","    return dataframe.filter(dataframe[WEATHER_CONDITION_COL] == class_name).count()\n","\n","def get_undersampling_fracs(dataframe: DataFrame) -> Dict[str, float]:\n","    rainy_cnt = count_weather_condition_occurrences(dataframe, 'rainy')\n","    snowy_cnt = count_weather_condition_occurrences(dataframe, 'snowy')\n","    sunny_cnt = count_weather_condition_occurrences(dataframe, 'sunny')\n","    foggy_cnt = count_weather_condition_occurrences(dataframe, 'foggy')\n","    cloudy_cnt = count_weather_condition_occurrences(dataframe, 'cloudy')\n","    thunderstorm_cnt = count_weather_condition_occurrences(dataframe, 'thunderstorm')\n","\n","    minority_class_cnt = np.min([rainy_cnt, snowy_cnt, sunny_cnt, cloudy_cnt, foggy_cnt, thunderstorm_cnt])\n","\n","    return {\n","        'rainy': minority_class_cnt / rainy_cnt,\n","        'snowy': minority_class_cnt / snowy_cnt,\n","        'sunny': minority_class_cnt / sunny_cnt,\n","        'foggy': minority_class_cnt / foggy_cnt,\n","        'cloudy': minority_class_cnt / cloudy_cnt,\n","        'thunderstorm': minority_class_cnt / thunderstorm_cnt\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sampled_weather_measurements_df = not_null_weather_measurements_df.sampleBy(\n","    WEATHER_CONDITION_COL,\n","    fractions = get_undersampling_fracs(not_null_weather_measurements_df),\n","    seed = 42)"]},{"cell_type":"markdown","metadata":{},"source":["Save the undersampled dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sampled_weather_measurements_df.write.csv('tmp',\n","    mode = 'overwrite',\n","    header = True)"]},{"cell_type":"markdown","metadata":{},"source":["### Data statistics"]},{"cell_type":"markdown","metadata":{},"source":["Load data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get all the csv files in the aggregated dataset folder\n","csv_files = [file for file in os.listdir(AGGREGATED_DATASET_PATH) if file.endswith('.csv')]\n","\n","# Read each CSV file into a Koalas DataFrame and store them in a list\n","dfs = [ps.read_csv(os.path.join(AGGREGATED_DATASET_PATH, file)) for file in csv_files]\n","\n","# Combine the DataFrames using the concat function\n","data = ps.concat(dfs, ignore_index = True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["features = [\n","    'humidity',\n","    'pressure',\n","    'temperature',\n","    'wind_direction',\n","    'wind_speed',\n","    'latitude',\n","    'longitude',\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data.head()"]},{"cell_type":"markdown","metadata":{},"source":["Descriptive statistics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data[features].describe().transpose()"]},{"cell_type":"markdown","metadata":{},"source":["Boxplot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["features = ['humidity', 'pressure', 'temperature']\n","\n","# Plot the data\n","fig, axs = plt.subplots(1, 3, figsize = (15, 5))\n","\n","axs[0].boxplot(data[features[0]])\n","axs[0].set_ylabel('value')\n","axs[0].set_title(features[0], fontweight = \"bold\")\n","\n","axs[1].boxplot(data[features[1]])\n","axs[1].set_ylabel('value')\n","axs[1].set_title(features[1], fontweight = \"bold\")\n","\n","axs[2].boxplot(data[features[2]])\n","axs[2].set_ylabel('value')\n","axs[2].set_title(features[2], fontweight = \"bold\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Distribution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["features = ['humidity', 'pressure', 'temperature']\n","f1_list = data[features[0]].tolist()\n","f2_list = data[features[1]].tolist()\n","f3_list = data[features[2]].tolist()\n","\n","# Plot the data\n","fig, axs = plt.subplots(1, 3, figsize = (15, 5))\n","\n","axs[0].hist2d(f1_list, f2_list, bins = (50, 50), vmax = 200)\n","axs[0].set_xlabel(features[0])\n","axs[0].set_ylabel(features[1])\n","axs[0].set_title(features[0] + ' vs ' + features[1], fontweight = \"bold\")\n","\n","axs[1].hist2d(f1_list, f3_list, bins = (50, 50), vmax = 200)\n","axs[1].set_xlabel(features[0])\n","axs[1].set_ylabel(features[2])\n","axs[1].set_title(features[0] + ' vs ' + features[2], fontweight = \"bold\")\n","\n","axs[2].hist2d(f2_list, f3_list, bins = (50, 50), vmax = 200)\n","axs[2].set_xlabel(features[1])\n","axs[2].set_ylabel(features[2])\n","axs[2].set_title(features[1] + ' vs ' + features[2], fontweight = \"bold\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Machine Learning pipeline"]},{"cell_type":"markdown","metadata":{},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import requests\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","\n","mpl.rcParams['figure.figsize'] = (8, 6)\n","mpl.rcParams['axes.grid'] = False\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.utils.multiclass import unique_labels\n","\n","import findspark\n","findspark.init()\n","findspark.find() \n","\n","import pyspark.pandas as ps\n","from pyspark.ml import Pipeline\n","from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler, IndexToString\n","from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, DecisionTreeClassifier, MultilayerPerceptronClassifier\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DATASET_PATH = 'datasets/historical-hourly-weather-dataset/'\n","AGGREGATED_DATASET_PATH = 'datasets/historical-hourly-weather-dataset/aggregated_sampled_weather_measurements'"]},{"cell_type":"markdown","metadata":{},"source":["### Load data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get all the csv files in the aggregated dataset folder\n","csv_files = [file for file in os.listdir(AGGREGATED_DATASET_PATH) if file.endswith('.csv')]\n","\n","# Read each CSV file into a Koalas DataFrame and store them in a list\n","dfs = [ps.read_csv(os.path.join(AGGREGATED_DATASET_PATH, file)) for file in csv_files]\n","\n","# Combine the DataFrames using the concat function\n","data = ps.concat(dfs, ignore_index = True)"]},{"cell_type":"markdown","metadata":{},"source":["### Pre-processing"]},{"cell_type":"markdown","metadata":{},"source":["Select relevant features and label column"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Select relevant features\n","numerical_cols = [\n","    'humidity',\n","    'pressure',\n","    'temperature',\n","    'wind_direction',\n","    'wind_speed',\n","    'latitude',\n","    'longitude'\n","]\n","nominal_cols = []\n","# Select the label column\n","label_col = 'weather_condition'\n","prediction_col = 'predicted_weather_condition'\n","# Select the features and the label\n","df_selected = data[numerical_cols + nominal_cols + [label_col]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_selected.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(df_selected)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get number of samples for each weather condition\n","tmp = df_selected[label_col].value_counts()\n","values = []\n","for i in range(len(tmp)):\n","    values.append(tmp[i])\n","    \n","# Names for each bar\n","bar_names = ['sunny', 'cloudy', 'snowy', 'foggy', 'thunderstorm', 'rainy']\n","\n","# Bar plot with custom names\n","plt.bar(bar_names, values, color = 'blue')\n","\n","# Adding labels and title\n","plt.xlabel('Labels')\n","plt.ylabel('Number of sampples')\n","\n","plt.savefig('bar_plot.png')\n","\n","# Display the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the Koalas DataFrame to a Spark DataFrame\n","df_selected = df_selected.to_spark()"]},{"cell_type":"markdown","metadata":{},"source":["Train-Test split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data, test_data = df_selected.randomSplit([0.8, 0.2], seed = 42)"]},{"cell_type":"markdown","metadata":{},"source":["Encode"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def encode(\n","    df,\n","    numerical_cols = [],\n","    nominal_cols = [],\n","    label_col = '',\n","    with_std = True,\n","    with_mean = True,\n","):\n","    # Convert categorical label to numerical label\n","    label_indexer = StringIndexer(\n","        inputCol = label_col,\n","        outputCol = 'label',\n","        handleInvalid = 'keep'\n","    )\n","    \n","    # Assemble features into a vector\n","    feature_cols = numerical_cols + nominal_cols\n","    vector_assembler = VectorAssembler(\n","        inputCols = feature_cols,\n","        outputCol = 'raw_features'\n","    )\n","    \n","    # Scale the features\n","    scaler = StandardScaler(\n","        inputCol = 'raw_features',\n","        outputCol = 'scaled_features',\n","        withStd = with_std,\n","        withMean = with_mean\n","    )\n","    \n","    stages = [label_indexer, vector_assembler, scaler]\n","    pipeline = Pipeline(stages = stages)\n","    \n","    transformer = pipeline.fit(df)\n","    \n","    return transformer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_encoder = encode(\n","    df = df_selected,\n","    numerical_cols = numerical_cols,\n","    nominal_cols = nominal_cols,\n","    label_col = label_col\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Weather Forecasting models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","    Support functions for training and evaluating the model\n","\"\"\"\n","\n","# Evaluate the model\n","def evaluate(predictions):\n","    accuracy = MulticlassClassificationEvaluator(\n","        labelCol = 'label',\n","        predictionCol = 'prediction',\n","        metricName = 'accuracy'\n","    )\n","    precision = MulticlassClassificationEvaluator(\n","        labelCol = 'label',\n","        predictionCol = 'prediction',\n","        metricName = 'weightedPrecision'\n","    )\n","    recall = MulticlassClassificationEvaluator(\n","        labelCol = 'label',\n","        predictionCol = 'prediction',\n","        metricName = 'weightedRecall'\n","    )\n","    f1 = MulticlassClassificationEvaluator(\n","        labelCol = 'label',\n","        predictionCol = 'prediction',\n","        metricName = 'f1'\n","    )\n","    \n","    print('Accuracy:', accuracy.evaluate(predictions))\n","    print('Precision:', precision.evaluate(predictions))\n","    print('Recall:', recall.evaluate(predictions))\n","    print('F1:', f1.evaluate(predictions))\n","    \n","# Confusion matrix\n","def plot_confusion_matrix(predictions_df, normalize: bool = False, title: str = None): \n","    labels = predictions_df.select(label_col).to_koalas().to_numpy()\n","    pedictions = predictions_df.select(prediction_col).to_koalas().to_numpy()\n","\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(labels, pedictions, labels=None)\n","    \n","    # Only use the labels that appear in the data\n","    classes = unique_labels(labels, pedictions)\n","\n","    if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(cm, interpolation='nearest', cmap = plt.cm.Blues)\n","    ax.figure.colorbar(im, ax=ax)\n","    \n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,           \n","           xlabel='Predicted label', ylabel='True label',\n","           title=title)\n","\n","    ax.set_ylim(len(classes) - 0.5, -0.5)\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation = 45, ha = 'right', rotation_mode = 'anchor')\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(\n","                j, i, format(cm[i, j], fmt), ha = 'center', va = 'center',\n","                color = 'white' if cm[i, j] > thresh else 'black'\n","            )\n","\n","    fig.tight_layout()\n","    plt.show()\n","    \n","# Index to string\n","idx_to_str = IndexToString(\n","    inputCol = 'prediction',\n","    outputCol = prediction_col,\n","    labels = data_encoder.stages[0].labels\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Random Forest"]},{"cell_type":"markdown","metadata":{},"source":["Define the classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier = RandomForestClassifier(\n","    featuresCol = 'scaled_features',\n","    labelCol = 'label',\n","    numTrees = 10,\n","    maxDepth = 50,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Define the pipeline with the encoding and classifier stages"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipeline = Pipeline(stages = [data_encoder, classifier])"]},{"cell_type":"markdown","metadata":{},"source":["Define the evaluator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluator = MulticlassClassificationEvaluator(\n","    labelCol = 'label',\n","    predictionCol = 'prediction',\n","    metricName = 'accuracy'\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Define hyperparameter tuning (optional)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the grid of hyperparameters\n","param_grid = ParamGridBuilder().build()\n","\n","# Set up the cross validator for model training and hyperparameter tuning\n","cross_validator = CrossValidator(\n","    estimator = pipeline,\n","    estimatorParamMaps = param_grid,\n","    evaluator = evaluator,\n","    numFolds = 5\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Fit the model using the training data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = cross_validator.fit(train_data)"]},{"cell_type":"markdown","metadata":{},"source":["Predict and evaluate on test data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = model.transform(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluate(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = idx_to_str.transform(predictions)"]},{"cell_type":"markdown","metadata":{},"source":["Confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_confusion_matrix(predictions, normalize = True)"]},{"cell_type":"markdown","metadata":{},"source":["### Logistic Regression"]},{"cell_type":"markdown","metadata":{},"source":["Define the classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier = LogisticRegression(\n","    featuresCol = 'scaled_features',\n","    labelCol = 'label',\n","    maxIter = 1000,\n","    regParam = 0.0,\n","    elasticNetParam = 0.0\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Define the pipeline with the encoding and classifier stages"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipeline = Pipeline(stages = [data_encoder, classifier])"]},{"cell_type":"markdown","metadata":{},"source":["Define the evaluator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluator = MulticlassClassificationEvaluator(\n","    labelCol = 'label',\n","    predictionCol = 'prediction',\n","    metricName = 'accuracy'\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Define hyperparameter tuning (optional)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the grid of hyperparameters\n","param_grid = ParamGridBuilder().build()\n","\n","# Set up the cross validator for model training and hyperparameter tuning\n","cross_validator = CrossValidator(\n","    estimator = pipeline,\n","    estimatorParamMaps = param_grid,\n","    evaluator = evaluator,\n","    numFolds = 5\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Fit the model using the training data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = cross_validator.fit(train_data)"]},{"cell_type":"markdown","metadata":{},"source":["Predict and evaluate on test data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = model.transform(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluate(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = idx_to_str.transform(predictions)"]},{"cell_type":"markdown","metadata":{},"source":["Confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_confusion_matrix(predictions, normalize = True)"]},{"cell_type":"markdown","metadata":{},"source":["### Decision Tree"]},{"cell_type":"markdown","metadata":{},"source":["Define the classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier = DecisionTreeClassifier(\n","    featuresCol = 'scaled_features',\n","    labelCol = 'label',\n","    maxDepth = 50\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Define the pipeline with the encoding and classifier stages"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipeline = Pipeline(stages = [data_encoder, classifier])"]},{"cell_type":"markdown","metadata":{},"source":["Define the evaluator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluator = MulticlassClassificationEvaluator(\n","    labelCol = 'label',\n","    predictionCol = 'prediction',\n","    metricName = 'accuracy'\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Define hyperparameter tuning (optional)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the grid of hyperparameters\n","param_grid = ParamGridBuilder().build()\n","\n","# Set up the cross validator for model training and hyperparameter tuning\n","cross_validator = CrossValidator(\n","    estimator = pipeline,\n","    estimatorParamMaps = param_grid,\n","    evaluator = evaluator,\n","    numFolds = 5\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Fit the model using the training data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = cross_validator.fit(train_data)"]},{"cell_type":"markdown","metadata":{},"source":["Predict and evaluate on test data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = model.transform(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluate(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = idx_to_str.transform(predictions)"]},{"cell_type":"markdown","metadata":{},"source":["Confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_confusion_matrix(predictions, normalize = True)"]},{"cell_type":"markdown","metadata":{},"source":["### Multilayer Perceptron"]},{"cell_type":"markdown","metadata":{},"source":["Define the layers of the neural network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["layers = [len(numerical_cols) + len(nominal_cols), 32, 64, 128, 6]"]},{"cell_type":"markdown","metadata":{},"source":["Define the classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier = MultilayerPerceptronClassifier(\n","    featuresCol = 'scaled_features',\n","    labelCol = 'label',\n","    maxIter = 500,\n","    layers = layers,\n","    blockSize = 128,\n","    seed = 42,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Define the pipeline with the encoding and classifier stages"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipeline = Pipeline(stages = [data_encoder, classifier])"]},{"cell_type":"markdown","metadata":{},"source":["Define the evaluator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluator = MulticlassClassificationEvaluator(\n","    labelCol = 'label',\n","    predictionCol = 'prediction',\n","    metricName = 'accuracy'\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Define hyperparameter tuning (optional)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the grid of hyperparameters\n","param_grid = ParamGridBuilder().build()\n","\n","# Set up the cross validator for model training and hyperparameter tuning\n","cross_validator = CrossValidator(\n","    estimator = pipeline,\n","    estimatorParamMaps = param_grid,\n","    evaluator = evaluator,\n","    numFolds = 5\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Fit the model using the training data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = cross_validator.fit(train_data)"]},{"cell_type":"markdown","metadata":{},"source":["Predict and evaluate on test data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = model.transform(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluate(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = idx_to_str.transform(predictions)"]},{"cell_type":"markdown","metadata":{},"source":["Confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_confusion_matrix(predictions, normalize = True)"]},{"cell_type":"markdown","metadata":{},"source":["# Comparison with [OpenWeather](https://openweathermap.org/) forecasts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def handle_description(desc: str):\n","    weather_condition_lowered = desc.lower()\n","\n","    if any(key in weather_condition_lowered for key in ['squall', 'thunderstorm']):\n","        desc = 'thunderstorm'\n","    elif any(key in weather_condition_lowered for key in ['drizzle', 'rain', \"rainy\"]):\n","        desc = 'rainy'\n","    elif any(key in weather_condition_lowered for key in ['sleet', 'snow', \"snowy\"]):\n","        desc = 'snowy'\n","    elif any(key in weather_condition_lowered for key in ['cloud', \"overcast\", \"cloudy\", \"clouds\"]):\n","        desc = 'cloudy'\n","    elif any(key in weather_condition_lowered for key in ['fog', 'mist', 'haze', \"smoke\", \"dust\", \"foggy\"]):\n","        desc = 'foggy'\n","    elif any(key in weather_condition_lowered for key in ['clear', 'sun', \"sunny\", ]):\n","        desc = 'sunny'\n","    return desc\n","\n","def handle_append_data(json, humidity_arr, time_arr, wind_dir_arr, wind_speed_arr, pressure_arr, weather_desc_arr , temp_arr):\n","\n","    for day_data in json[\"data\"][\"weather\"]:\n","\n","        date = day_data[\"date\"]\n","        for hour_data in day_data['hourly']:\n","            time = \"{date} {time}:00:00\".format(date = date, time = int(int(hour_data[\"time\"])/100))\n","            wind_speed_ms = round(float(hour_data[\"windspeedKmph\"])* (10/36),2)\n","            wind_dir = int(hour_data[\"winddirDegree\"])\n","            humidity = int(hour_data[\"humidity\"])\n","            pressure = int(hour_data[\"pressure\"])\n","            tempK = int(hour_data[\"tempC\"]) + 273.15\n","            weather_desc = hour_data[\"weatherDesc\"][0][\"value\"]\n","            weather_desc = handle_description(weather_desc)\n","\n","            time_arr = np.append(time_arr, time)\n","            humidity_arr = np.append(humidity_arr ,  humidity)\n","            wind_speed_arr = np.append(wind_speed_arr, wind_speed_ms)\n","            wind_dir_arr = np.append(wind_dir_arr, wind_dir)\n","            pressure_arr = np.append(pressure_arr, pressure)\n","            temp_arr = np.append(temp_arr, tempK)\n","            weather_desc_arr = np.append(weather_desc_arr, weather_desc)\n","\n","    return humidity_arr, time_arr, wind_dir_arr, wind_speed_arr, pressure_arr, weather_desc_arr , temp_arr\n","\n","def get_7_days_forecasts_df(lng = 10.762622,lat = 106.660172, city_name = \"Ho Chi Minh City\", api_key = 'c360b29aaa5a41a894b130731230511'):\n","    link = \"http://api.worldweatheronline.com/premium/v1/marine.ashx?q={lng},{lat}&key={api_key}&format=json&tp=3\".format(\n","        lng = lng,\n","        lat = lat,\n","        api_key = api_key,\n","    )\n","    res = requests.get(link).json()\n","\n","    humidity_arr = np.array([])\n","    time_arr = np.array([])\n","    wind_dir_arr = np.array([])\n","    wind_speed_arr = np.array([])\n","    pressure_arr = np.array([])\n","    temp_arr = np.array([])\n","    weather_desc_arr = np.array([], dtype = object)\n","\n","    humidity_arr, time_arr, wind_dir_arr, wind_speed_arr, pressure_arr, weather_desc_arr , temp_arr = handle_append_data(\n","            res, humidity_arr, time_arr, wind_dir_arr, wind_speed_arr, pressure_arr, weather_desc_arr , temp_arr)\n","    df = ps.DataFrame({'datetime': time_arr, 'humidity': humidity_arr,'temperature': temp_arr,'pressure': pressure_arr,'wind_direction': wind_dir_arr,'wind_speed': wind_speed_arr,'weather_condition': weather_desc_arr,'city': city_name,'latitude':float(lat),'longitude':float(lng)})\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["worldweather_forecast = get_7_days_forecasts_df(49.24966, 123.119339, \"Vancouver\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_test = worldweather_forecast[numerical_cols + nominal_cols + [label_col]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_test = df_selected.to_spark()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["openweather_forecasts_predictions = model.transform(df_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = idx_to_str.transform(openweather_forecasts_predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluate(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_confusion_matrix(predictions, normalize = True)"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"Weather_forecasting_pyspark","notebookOrigID":1252952709241176,"widgets":{}},"kernelspec":{"display_name":"weather","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
